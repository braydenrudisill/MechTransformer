{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the custom dictionary for our vocabs\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path(\"data/MIT_separated_augm/src-train.txt\")) as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "    tokens = sorted(set(chain.from_iterable(map(str.split, map(str.strip, lines)))))\n",
    "\n",
    "    # Here I moved the SMILE vocabs back by 4 spaces in the dictionary\n",
    "    # In order to add the following tokens into the dictionary\n",
    "    dictionary = dict(zip(tokens, range(4, len(tokens) + 4)))\n",
    "    dictionary[\"[START]\"] = 0\n",
    "    dictionary[\"[END]\"] = 1\n",
    "    dictionary[\"[PAD]\"] = 2\n",
    "    dictionary[\"[UNK]\"] = 3\n",
    "\n",
    "\n",
    "# Making the tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "tokenizer = Tokenizer(WordLevel(vocab = dictionary, unk_token=\"[UNK]\"))\n",
    "tokenizer.save(\"tokenizer_from_new_mayr.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the tokenizer from data in two files...\n",
    "# Dataloading\n",
    "# In the data_files={...}, specify train, val, and test\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "# %%\n",
    "# Make the custom dictionary for our vocabs\n",
    "from itertools import chain\n",
    "with open(Path(\"data/MIT_separated_augm/src-train.txt\")) as file_1:\n",
    "    lines_1 = file_1.readlines()\n",
    "\n",
    "with open(Path(\"data/MIT_separated_augm/tgt-train.txt\")) as file_2:\n",
    "    lines_2 = file_2.readlines()\n",
    "\n",
    "    tokens = sorted(set(chain.from_iterable(map(str.split, map(str.strip, lines_1 + lines_2)))))\n",
    "    tokens_1 = set(chain.from_iterable(map(str.split, map(str.strip, lines_1))))\n",
    "    tokens_2 = set(chain.from_iterable(map(str.split, map(str.strip, lines_2))))\n",
    "\n",
    "    # Here I moved the SMILE vocabs back by 4 spaces in the dictionary\n",
    "    # In order to add the following tokens into the dictionary\n",
    "    dictionary = dict(zip(tokens, range(4, len(tokens) + 4)))\n",
    "    dictionary[\"[START]\"] = 0\n",
    "    dictionary[\"[END]\"] = 1\n",
    "    dictionary[\"[PAD]\"] = 2\n",
    "    dictionary[\"[UNK]\"] = 3\n",
    "\n",
    "# %%\n",
    "# Making the tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "tokenizer = Tokenizer(WordLevel(vocab = dictionary, unk_token=\"[UNK]\"))\n",
    "tokenizer.save(\"tokenizer_from_USPTO_MIT.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "298"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in the dictionary are unique.\n"
     ]
    }
   ],
   "source": [
    "values_list = list(dictionary.values())\n",
    "\n",
    "# Check if the length of the list is equal to the length of the set of values\n",
    "if len(values_list) == len(set(values_list)):\n",
    "    print(\"Values in the dictionary are unique.\")\n",
    "else:\n",
    "    print(\"Values in the dictionary are not unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinxing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
